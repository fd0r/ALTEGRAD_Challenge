{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First tries\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import codecs\n",
    "from os import path\n",
    "import csv\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 2125/2125 [00:00<00:00, 708609.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28002\n",
      "319498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28003/28003 [00:19<00:00, 1462.62it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2125/2125 [00:00<00:00, 1063718.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# Read training data\n",
    "with open(\"train.csv\", 'r') as f:\n",
    "    train_data = f.read().splitlines()\n",
    "\n",
    "train_hosts = list()\n",
    "y_train = list()\n",
    "for row in tqdm(train_data):\n",
    "    host, label = row.split(\",\")\n",
    "    train_hosts.append(host)\n",
    "    y_train.append(label.lower())\n",
    "\n",
    "# Read test data\n",
    "with open(\"test.csv\", 'r') as f:\n",
    "    test_hosts = f.read().splitlines()\n",
    "\n",
    "# Create a directed, weighted graph\n",
    "G = nx.read_weighted_edgelist('edgelist.txt', create_using=nx.DiGraph())\n",
    "\n",
    "print(G.number_of_nodes())\n",
    "print(G.number_of_edges())\n",
    "\n",
    "# Load the textual content of a set of webpages for each host into the dictionary \"text\". \n",
    "# The encoding parameter is required since the majority of our text is french.\n",
    "text = dict()\n",
    "filenames = os.listdir('text/text')\n",
    "for filename in tqdm(filenames):\n",
    "    with codecs.open(path.join('text/text/', filename), encoding='latin-1') as f: \n",
    "        text[filename] = f.read().replace(\"\\n\", \"\").lower()\n",
    "\n",
    "train_data = list()\n",
    "for host in tqdm(train_hosts):\n",
    "    if host in text:\n",
    "        train_data.append(text[host])\n",
    "    else:\n",
    "        train_data.append('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text approach only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 560/560 [00:00<00:00, 560307.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train matrix dimensionality:  (2125, 21384)\n",
      "Test matrix dimensionality:  (560, 21384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.8min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.8min finished\n"
     ]
    }
   ],
   "source": [
    "# Create the training matrix. Each row corresponds to a web host and each column to a word present in at least 10 web\n",
    "# hosts and at most 1000 web hosts. The value of each entry in a row is equal to the tf-idf weight of that word in the \n",
    "# corresponding web host       \n",
    "\n",
    "vec = TfidfVectorizer(\n",
    "    decode_error='ignore', strip_accents='unicode', encoding='latin-1', \n",
    "    min_df=10, max_df=1000)\n",
    "X_train = vec.fit_transform(train_data)\n",
    "\n",
    "# Get textual content of web hosts of the test set\n",
    "test_data = list()\n",
    "for host in tqdm(test_hosts):\n",
    "    if host in text:\n",
    "        test_data.append(text[host])\n",
    "    else:\n",
    "        test_data.append('')\n",
    "\n",
    "# Create the test matrix following the same approach as in the case of the training matrix\n",
    "X_test = vec.transform(test_data)\n",
    "\n",
    "print(\"Train matrix dimensionality: \", X_train.shape)\n",
    "print(\"Test matrix dimensionality: \", X_test.shape)\n",
    "\n",
    "# Use logistic regression to classify the webpages of the test set\n",
    "clf = LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=1000, verbose=5)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "\n",
    "# Write predictions to a file\n",
    "with open('text_baseline.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "    lst = clf.classes_.tolist()\n",
    "    lst.insert(0, \"Host\")\n",
    "    writer.writerow(lst)\n",
    "    for i,test_host in enumerate(test_hosts):\n",
    "        lst = y_pred[i,:].tolist()\n",
    "        lst.insert(0, test_host)\n",
    "        writer.writerow(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph approach only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train matrix dimensionality:  (2125, 3)\n",
      "Test matrix dimensionality:  (560, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.1s finished\n"
     ]
    }
   ],
   "source": [
    "# Create the training matrix. Each row corresponds to a web host.\n",
    "# Use the following 3 features for each web host (unweighted degrees)\n",
    "# (1) out-degree of node\n",
    "# (2) in-degree of node\n",
    "# (3) average degree of neighborhood of node\n",
    "X_train = np.zeros((len(train_hosts), 3))\n",
    "avg_neig_deg = nx.average_neighbor_degree(G, nodes=train_hosts)\n",
    "for i in range(len(train_hosts)):\n",
    "    X_train[i,0] = G.in_degree(train_hosts[i])\n",
    "    X_train[i,1] = G.out_degree(train_hosts[i])\n",
    "    X_train[i,2] = avg_neig_deg[train_hosts[i]]\n",
    "\n",
    "# Create the test matrix. Use the same 3 features as above\n",
    "X_test = np.zeros((len(test_hosts), 3))\n",
    "avg_neig_deg = nx.average_neighbor_degree(G, nodes=test_hosts)\n",
    "for i in range(len(test_hosts)):\n",
    "    X_test[i,0] = G.in_degree(test_hosts[i])\n",
    "    X_test[i,1] = G.out_degree(test_hosts[i])\n",
    "    X_test[i,2] = avg_neig_deg[test_hosts[i]]\n",
    "\n",
    "print(\"Train matrix dimensionality: \", X_train.shape)\n",
    "print(\"Test matrix dimensionality: \", X_test.shape)\n",
    "\n",
    "# Use logistic regression to classify the webpages of the test set\n",
    "clf = LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=2000, verbose=True)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "\n",
    "# Write predictions to a file\n",
    "with open('graph_baseline.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "    lst = clf.classes_.tolist()\n",
    "    lst.insert(0, \"Host\")\n",
    "    writer.writerow(lst)\n",
    "    for i,test_host in enumerate(test_hosts):\n",
    "        lst = y_pred[i,:].tolist()\n",
    "        lst.insert(0, test_host)\n",
    "        writer.writerow(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mix Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_prediction(y_pred, name=\"baseline\"):\n",
    "    with open('{}.csv'.format(name), 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',')\n",
    "        lst = clf.classes_.tolist()\n",
    "        lst.insert(0, \"Host\")\n",
    "        writer.writerow(lst)\n",
    "        for i,test_host in enumerate(test_hosts):\n",
    "            lst = y_pred[i,:].tolist()\n",
    "            lst.insert(0, test_host)\n",
    "            writer.writerow(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 560/560 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create the training matrix. Each row corresponds to a web host and each column to a word present in at least 10 web\n",
    "# hosts and at most 1000 web hosts. The value of each entry in a row is equal to the tf-idf weight of that word in the \n",
    "# corresponding web host\n",
    "vec = TfidfVectorizer(decode_error='ignore', strip_accents='unicode', encoding='latin-1', min_df=10, max_df=1000)\n",
    "X_train_text = vec.fit_transform(train_data)\n",
    "\n",
    "# Get textual content of web hosts of the test set\n",
    "test_data = list()\n",
    "for host in tqdm(test_hosts):\n",
    "    if host in text:\n",
    "        test_data.append(text[host])\n",
    "    else:\n",
    "        test_data.append('')\n",
    "\n",
    "\n",
    "# Create the test matrix following the same approach as in the case of the training matrix\n",
    "X_test_text = vec.transform(test_data)\n",
    "# Create the training matrix. Each row corresponds to a web host.\n",
    "# Use the following 3 features for each web host (unweighted degrees)\n",
    "# (1) out-degree of node\n",
    "# (2) in-degree of node\n",
    "# (3) average degree of neighborhood of node\n",
    "X_train_graph = np.zeros((len(train_hosts), 3))\n",
    "avg_neig_deg = nx.average_neighbor_degree(G, nodes=train_hosts)\n",
    "for i in range(len(train_hosts)):\n",
    "    X_train_graph[i,0] = G.in_degree(train_hosts[i])\n",
    "    X_train_graph[i,1] = G.out_degree(train_hosts[i])\n",
    "    X_train_graph[i,2] = avg_neig_deg[train_hosts[i]]\n",
    "\n",
    "# Create the test matrix. Use the same 3 features as above\n",
    "X_test_graph = np.zeros((len(test_hosts), 3))\n",
    "avg_neig_deg = nx.average_neighbor_degree(G, nodes=test_hosts)\n",
    "for i in range(len(test_hosts)):\n",
    "    X_test_graph[i,0] = G.in_degree(test_hosts[i])\n",
    "    X_test_graph[i,1] = G.out_degree(test_hosts[i])\n",
    "    X_test_graph[i,2] = avg_neig_deg[test_hosts[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2125, 3), (2125, 21384))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_graph.shape, X_train_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, scipy.sparse.csr.csr_matrix)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_graph), type(X_train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train_graph, np.array(X_train_text.todense())), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.concatenate((X_test_graph, np.array(X_test_text.todense())), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use logistic regression to classify the webpages of the test set\n",
    "clf = LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=2000, verbose=True)\n",
    "clf.fit(X_train, y_train)\n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "predict_proba is not available when  probability=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-e5574c5f9363>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mclf_svc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_train_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf_svc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf_svc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    634\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m         \"\"\"\n\u001b[1;32m--> 636\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict_proba\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_check_proba\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    601\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobability\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m             raise AttributeError(\"predict_proba is not available when \"\n\u001b[0m\u001b[0;32m    604\u001b[0m                                  \" probability=False\")\n\u001b[0;32m    605\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'c_svc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'nu_svc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: predict_proba is not available when  probability=False"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "# Use logistic regression to classify the webpages of the test set\n",
    "clf_svc = SVC(verbose=True)\n",
    "clf_svc.fit(X_train, y_train)\n",
    "y_train_pred = clf_svc.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luism\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         precision    recall  f1-score   support\n",
      "\n",
      "       business/finance       0.31      0.94      0.46       626\n",
      "     education/research       1.00      0.00      0.01       209\n",
      "          entertainment       0.35      0.10      0.15       579\n",
      "         health/medical       0.00      0.00      0.00        92\n",
      "             news/press       1.00      0.01      0.02        83\n",
      "politics/government/law       0.43      0.09      0.15       200\n",
      "                 sports       0.00      0.00      0.00        46\n",
      "           tech/science       0.67      0.03      0.05       290\n",
      "\n",
      "               accuracy                           0.32      2125\n",
      "              macro avg       0.47      0.15      0.11      2125\n",
      "           weighted avg       0.45      0.32      0.20      2125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf_svc.predict(X_test)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   12.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         precision    recall  f1-score   support\n",
      "\n",
      "       business/finance       0.97      0.98      0.97       626\n",
      "     education/research       0.96      0.94      0.95       209\n",
      "          entertainment       0.97      0.98      0.97       579\n",
      "         health/medical       0.98      0.96      0.97        92\n",
      "             news/press       0.95      0.90      0.93        83\n",
      "politics/government/law       0.96      0.96      0.96       200\n",
      "                 sports       0.98      0.98      0.98        46\n",
      "           tech/science       0.96      0.94      0.95       290\n",
      "\n",
      "               accuracy                           0.97      2125\n",
      "              macro avg       0.97      0.96      0.96      2125\n",
      "           weighted avg       0.97      0.97      0.97      2125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Use logistic regression to classify the webpages of the test set\n",
    "clf_rf = RandomForestClassifier(verbose=True)\n",
    "clf_rf.fit(X_train, y_train)\n",
    "y_train_pred = clf_rf.predict(X_train)\n",
    "y_pred = clf_rf.predict(X_test)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class OverCertainClassifier(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.num_class = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.num_class = len(np.unique(y))\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return np.ones((X.shape[0], self.num_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_clf = OverCertainClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dummy_clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_prediction(y_pred, \"ones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
