{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, labels=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            labels: (Optional) [string]. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.labels = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers.modeling'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-f28be7c8daa9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodeling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPreTrainedBertModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mBertForMultiLabelSequenceClassification\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPreTrainedBertModel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \"\"\"BERT model for classification.\n\u001b[0;32m      4\u001b[0m     \u001b[0mThis\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mcomposed\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mBERT\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlinear\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0mon\u001b[0m \u001b[0mtop\u001b[0m \u001b[0mof\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mthe\u001b[0m \u001b[0mpooled\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers.modeling'"
     ]
    }
   ],
   "source": [
    "from transformers.modeling import PreTrainedBertModel\n",
    "class BertForMultiLabelSequenceClassification(PreTrainedBertModel):\n",
    "    \"\"\"BERT model for classification.\n",
    "    This module is composed of the BERT model with a linear layer on top of\n",
    "    the pooled output.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, num_labels=2):\n",
    "        super(BertForMultiLabelSequenceClassification, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size, num_labels)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1, self.num_labels))\n",
    "            return loss\n",
    "        else:\n",
    "            return logits\n",
    "        \n",
    "    def freeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package transformers:\n",
      "\n",
      "NAME\n",
      "    transformers\n",
      "\n",
      "DESCRIPTION\n",
      "    # flake8: noqa\n",
      "    # There's no way to ignore \"F401 '...' imported but unused\" warnings in this\n",
      "    # module, but to preserve other warnings. So, don't check this module at all.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    __main__\n",
      "    commands (package)\n",
      "    configuration_albert\n",
      "    configuration_auto\n",
      "    configuration_bert\n",
      "    configuration_camembert\n",
      "    configuration_ctrl\n",
      "    configuration_distilbert\n",
      "    configuration_flaubert\n",
      "    configuration_gpt2\n",
      "    configuration_mmbt\n",
      "    configuration_openai\n",
      "    configuration_roberta\n",
      "    configuration_t5\n",
      "    configuration_transfo_xl\n",
      "    configuration_utils\n",
      "    configuration_xlm\n",
      "    configuration_xlm_roberta\n",
      "    configuration_xlnet\n",
      "    convert_albert_original_tf_checkpoint_to_pytorch\n",
      "    convert_bert_original_tf_checkpoint_to_pytorch\n",
      "    convert_bert_pytorch_checkpoint_to_original_tf\n",
      "    convert_gpt2_original_tf_checkpoint_to_pytorch\n",
      "    convert_openai_original_tf_checkpoint_to_pytorch\n",
      "    convert_pytorch_checkpoint_to_tf2\n",
      "    convert_roberta_original_pytorch_checkpoint_to_pytorch\n",
      "    convert_t5_original_tf_checkpoint_to_pytorch\n",
      "    convert_transfo_xl_original_tf_checkpoint_to_pytorch\n",
      "    convert_xlm_original_pytorch_checkpoint_to_pytorch\n",
      "    convert_xlnet_original_tf_checkpoint_to_pytorch\n",
      "    data (package)\n",
      "    file_utils\n",
      "    hf_api\n",
      "    modelcard\n",
      "    modeling_albert\n",
      "    modeling_auto\n",
      "    modeling_bert\n",
      "    modeling_camembert\n",
      "    modeling_ctrl\n",
      "    modeling_distilbert\n",
      "    modeling_encoder_decoder\n",
      "    modeling_flaubert\n",
      "    modeling_gpt2\n",
      "    modeling_mmbt\n",
      "    modeling_openai\n",
      "    modeling_roberta\n",
      "    modeling_t5\n",
      "    modeling_tf_albert\n",
      "    modeling_tf_auto\n",
      "    modeling_tf_bert\n",
      "    modeling_tf_camembert\n",
      "    modeling_tf_ctrl\n",
      "    modeling_tf_distilbert\n",
      "    modeling_tf_gpt2\n",
      "    modeling_tf_openai\n",
      "    modeling_tf_pytorch_utils\n",
      "    modeling_tf_roberta\n",
      "    modeling_tf_t5\n",
      "    modeling_tf_transfo_xl\n",
      "    modeling_tf_transfo_xl_utilities\n",
      "    modeling_tf_utils\n",
      "    modeling_tf_xlm\n",
      "    modeling_tf_xlm_roberta\n",
      "    modeling_tf_xlnet\n",
      "    modeling_transfo_xl\n",
      "    modeling_transfo_xl_utilities\n",
      "    modeling_utils\n",
      "    modeling_xlm\n",
      "    modeling_xlm_roberta\n",
      "    modeling_xlnet\n",
      "    optimization\n",
      "    optimization_tf\n",
      "    pipelines\n",
      "    tokenization_albert\n",
      "    tokenization_auto\n",
      "    tokenization_bert\n",
      "    tokenization_bert_japanese\n",
      "    tokenization_camembert\n",
      "    tokenization_ctrl\n",
      "    tokenization_distilbert\n",
      "    tokenization_flaubert\n",
      "    tokenization_gpt2\n",
      "    tokenization_openai\n",
      "    tokenization_roberta\n",
      "    tokenization_t5\n",
      "    tokenization_transfo_xl\n",
      "    tokenization_utils\n",
      "    tokenization_xlm\n",
      "    tokenization_xlm_roberta\n",
      "    tokenization_xlnet\n",
      "\n",
      "DATA\n",
      "    ALBERT_PRETRAINED_CONFIG_ARCHIVE_MAP = {'albert-base-v1': 'https://s3....\n",
      "    ALBERT_PRETRAINED_MODEL_ARCHIVE_MAP = {'albert-base-v1': 'https://s3.a...\n",
      "    ALL_PRETRAINED_CONFIG_ARCHIVE_MAP = {'albert-base-v1': 'https://s3.ama...\n",
      "    ALL_PRETRAINED_MODEL_ARCHIVE_MAP = {'albert-base-v1': 'https://s3.amaz...\n",
      "    BERT_PRETRAINED_CONFIG_ARCHIVE_MAP = {'bert-base-cased': 'https://s3.a...\n",
      "    BERT_PRETRAINED_MODEL_ARCHIVE_MAP = {'bert-base-cased': 'https://s3.am...\n",
      "    CAMEMBERT_PRETRAINED_CONFIG_ARCHIVE_MAP = {'camembert-base': 'https://...\n",
      "    CAMEMBERT_PRETRAINED_MODEL_ARCHIVE_MAP = {'camembert-base': 'https://s...\n",
      "    CONFIG_NAME = 'config.json'\n",
      "    CTRL_PRETRAINED_CONFIG_ARCHIVE_MAP = {'ctrl': 'https://storage.googlea...\n",
      "    CTRL_PRETRAINED_MODEL_ARCHIVE_MAP = {'ctrl': 'https://storage.googleap...\n",
      "    DISTILBERT_PRETRAINED_CONFIG_ARCHIVE_MAP = {'distilbert-base-german-ca...\n",
      "    DISTILBERT_PRETRAINED_MODEL_ARCHIVE_MAP = {'distilbert-base-german-cas...\n",
      "    FLAUBERT_PRETRAINED_CONFIG_ARCHIVE_MAP = {'flaubert-base-cased': 'http...\n",
      "    FLAUBERT_PRETRAINED_MODEL_ARCHIVE_MAP = {'flaubert-base-cased': 'https...\n",
      "    GPT2_PRETRAINED_CONFIG_ARCHIVE_MAP = {'distilgpt2': 'https://s3.amazon...\n",
      "    GPT2_PRETRAINED_MODEL_ARCHIVE_MAP = {'distilgpt2': 'https://s3.amazona...\n",
      "    MODEL_CARD_NAME = 'modelcard.json'\n",
      "    OPENAI_GPT_PRETRAINED_CONFIG_ARCHIVE_MAP = {'openai-gpt': 'https://s3....\n",
      "    OPENAI_GPT_PRETRAINED_MODEL_ARCHIVE_MAP = {'openai-gpt': 'https://s3.a...\n",
      "    PYTORCH_PRETRAINED_BERT_CACHE = WindowsPath('C:/Users/luism/.cache/tor...\n",
      "    PYTORCH_TRANSFORMERS_CACHE = WindowsPath('C:/Users/luism/.cache/torch/...\n",
      "    ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP = {'distilroberta-base': 'https:...\n",
      "    ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP = {'distilroberta-base': 'https:/...\n",
      "    SPIECE_UNDERLINE = '▁'\n",
      "    T5_PRETRAINED_CONFIG_ARCHIVE_MAP = {'t5-11b': 'https://s3.amazonaws.co...\n",
      "    T5_PRETRAINED_MODEL_ARCHIVE_MAP = {'t5-11b': 'https://s3.amazonaws.com...\n",
      "    TF2_WEIGHTS_NAME = 'tf_model.h5'\n",
      "    TF_ALBERT_PRETRAINED_MODEL_ARCHIVE_MAP = {'albert-base-v1': 'https://s...\n",
      "    TF_ALL_PRETRAINED_MODEL_ARCHIVE_MAP = {'albert-base-v1': 'https://s3.a...\n",
      "    TF_BERT_PRETRAINED_MODEL_ARCHIVE_MAP = {'bert-base-cased': 'https://s3...\n",
      "    TF_CAMEMBERT_PRETRAINED_MODEL_ARCHIVE_MAP = {}\n",
      "    TF_CTRL_PRETRAINED_MODEL_ARCHIVE_MAP = {'ctrl': 'https://s3.amazonaws....\n",
      "    TF_DISTILBERT_PRETRAINED_MODEL_ARCHIVE_MAP = {'distilbert-base-multili...\n",
      "    TF_GPT2_PRETRAINED_MODEL_ARCHIVE_MAP = {'distilgpt2': 'https://s3.amaz...\n",
      "    TF_OPENAI_GPT_PRETRAINED_MODEL_ARCHIVE_MAP = {'openai-gpt': 'https://s...\n",
      "    TF_ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP = {'distilroberta-base': 'http...\n",
      "    TF_T5_PRETRAINED_MODEL_ARCHIVE_MAP = {'t5-11b': 'https://s3.amazonaws....\n",
      "    TF_TRANSFO_XL_PRETRAINED_MODEL_ARCHIVE_MAP = {'transfo-xl-wt103': 'htt...\n",
      "    TF_WEIGHTS_NAME = 'model.ckpt'\n",
      "    TF_XLM_PRETRAINED_MODEL_ARCHIVE_MAP = {'xlm-clm-ende-1024': 'https://s...\n",
      "    TF_XLM_ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP = {}\n",
      "    TF_XLNET_PRETRAINED_MODEL_ARCHIVE_MAP = {'xlnet-base-cased': 'https://...\n",
      "    TRANSFORMERS_CACHE = WindowsPath('C:/Users/luism/.cache/torch/transfor...\n",
      "    TRANSFO_XL_PRETRAINED_CONFIG_ARCHIVE_MAP = {'transfo-xl-wt103': 'https...\n",
      "    TRANSFO_XL_PRETRAINED_MODEL_ARCHIVE_MAP = {'transfo-xl-wt103': 'https:...\n",
      "    WEIGHTS_NAME = 'pytorch_model.bin'\n",
      "    XLM_PRETRAINED_CONFIG_ARCHIVE_MAP = {'xlm-clm-ende-1024': 'https://s3....\n",
      "    XLM_PRETRAINED_MODEL_ARCHIVE_MAP = {'xlm-clm-ende-1024': 'https://s3.a...\n",
      "    XLM_ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP = {'xlm-roberta-base': 'http...\n",
      "    XLM_ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP = {'xlm-roberta-base': 'https...\n",
      "    XLNET_PRETRAINED_CONFIG_ARCHIVE_MAP = {'xlnet-base-cased': 'https://s3...\n",
      "    XLNET_PRETRAINED_MODEL_ARCHIVE_MAP = {'xlnet-base-cased': 'https://s3....\n",
      "    glue_output_modes = {'cola': 'classification', 'mnli': 'classification...\n",
      "    glue_processors = {'cola': <class 'transformers.data.processors.glue.C...\n",
      "    glue_tasks_num_labels = {'cola': 2, 'mnli': 3, 'mrpc': 2, 'qnli': 2, '...\n",
      "    logger = <Logger transformers (INFO)>\n",
      "    xnli_output_modes = {'xnli': 'classification'}\n",
      "    xnli_processors = {'xnli': <class 'transformers.data.processors.xnli.X...\n",
      "    xnli_tasks_num_labels = {'xnli': 3}\n",
      "\n",
      "VERSION\n",
      "    2.4.1\n",
      "\n",
      "FILE\n",
      "    c:\\users\\luism\\appdata\\roaming\\python\\python37\\site-packages\\transformers\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "help(transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
